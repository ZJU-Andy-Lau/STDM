nohup: 忽略输入
W1230 19:20:22.291000 2176478 site-packages/torch/distributed/run.py:774] 
W1230 19:20:22.291000 2176478 site-packages/torch/distributed/run.py:774] *****************************************
W1230 19:20:22.291000 2176478 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1230 19:20:22.291000 2176478 site-packages/torch/distributed/run.py:774] *****************************************
[rank1]:[W1230 19:20:26.285113167 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
============================================================
开始独立测试流程 | RUN ID: 20251228_231040
============================================================
[rank0]:[W1230 19:20:26.291069050 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.

>>> 正在评估: 2nd Best Val Loss Model
    路径: ./weights/st_diffusion_model_v2_20251228_231040_second_best.pth
Model loaded successfully.
Model loaded successfully.
Test dataset size: 433 samples.
Test dataset size: 433 samples.
Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]Evaluating:   4%|▎         | 1/28 [02:39<1:11:44, 159.42s/it]Evaluating:   7%|▋         | 2/28 [05:18<1:08:53, 158.97s/it]Evaluating:  11%|█         | 3/28 [07:56<1:06:10, 158.82s/it]Evaluating:  14%|█▍        | 4/28 [10:35<1:03:29, 158.74s/it]Evaluating:  18%|█▊        | 5/28 [13:13<1:00:50, 158.70s/it]Evaluating:  21%|██▏       | 6/28 [15:52<58:10, 158.67s/it]  Evaluating:  25%|██▌       | 7/28 [18:31<55:31, 158.65s/it]Evaluating:  29%|██▊       | 8/28 [21:09<52:52, 158.64s/it]Evaluating:  32%|███▏      | 9/28 [23:48<50:14, 158.63s/it]Evaluating:  36%|███▌      | 10/28 [26:27<47:35, 158.63s/it]Evaluating:  39%|███▉      | 11/28 [29:05<44:56, 158.62s/it]Evaluating:  43%|████▎     | 12/28 [31:44<42:17, 158.62s/it]Evaluating:  46%|████▋     | 13/28 [34:22<39:39, 158.62s/it]Evaluating:  50%|█████     | 14/28 [37:01<37:00, 158.62s/it]Evaluating:  54%|█████▎    | 15/28 [39:40<34:22, 158.63s/it]Evaluating:  57%|█████▋    | 16/28 [42:18<31:43, 158.63s/it]Evaluating:  61%|██████    | 17/28 [44:57<29:04, 158.63s/it]Evaluating:  64%|██████▍   | 18/28 [47:36<26:26, 158.63s/it]Evaluating:  68%|██████▊   | 19/28 [50:14<23:47, 158.64s/it]Evaluating:  71%|███████▏  | 20/28 [52:53<21:09, 158.64s/it]Evaluating:  75%|███████▌  | 21/28 [55:32<18:30, 158.64s/it]Evaluating:  79%|███████▊  | 22/28 [58:10<15:51, 158.57s/it]Evaluating:  82%|████████▏ | 23/28 [1:00:48<13:12, 158.53s/it]Evaluating:  86%|████████▌ | 24/28 [1:03:27<10:33, 158.50s/it]Evaluating:  89%|████████▉ | 25/28 [1:06:05<07:55, 158.48s/it]Evaluating:  93%|█████████▎| 26/28 [1:08:44<05:16, 158.46s/it]Evaluating:  96%|█████████▋| 27/28 [1:11:22<02:38, 158.45s/it]Rank 1 :idx list:[array([ 1,  3,  5,  7,  9, 11, 13, 15]), array([17, 19, 21, 23, 25, 27, 29, 31]), array([33, 35, 37, 39, 41, 43, 45, 47]), array([49, 51, 53, 55, 57, 59, 61, 63]), array([65, 67, 69, 71, 73, 75, 77, 79]), array([81, 83, 85, 87, 89, 91, 93, 95]), array([ 97,  99, 101, 103, 105, 107, 109, 111]), array([113, 115, 117, 119, 121, 123, 125, 127]), array([129, 131, 133, 135, 137, 139, 141, 143]), array([145, 147, 149, 151, 153, 155, 157, 159]), array([161, 163, 165, 167, 169, 171, 173, 175]), array([177, 179, 181, 183, 185, 187, 189, 191]), array([193, 195, 197, 199, 201, 203, 205, 207]), array([209, 211, 213, 215, 217, 219, 221, 223]), array([225, 227, 229, 231, 233, 235, 237, 239]), array([241, 243, 245, 247, 249, 251, 253, 255]), array([257, 259, 261, 263, 265, 267, 269, 271]), array([273, 275, 277, 279, 281, 283, 285, 287]), array([289, 291, 293, 295, 297, 299, 301, 303]), array([305, 307, 309, 311, 313, 315, 317, 319]), array([321, 323, 325, 327, 329, 331, 333, 335]), array([337, 339, 341, 343, 345, 347, 349, 351]), array([353, 355, 357, 359, 361, 363, 365, 367]), array([369, 371, 373, 375, 377, 379, 381, 383]), array([385, 387, 389, 391, 393, 395, 397, 399]), array([401, 403, 405, 407, 409, 411, 413, 415]), array([417, 419, 421, 423, 425, 427, 429, 431]), array([0])]
Evaluating: 100%|██████████| 28/28 [1:11:52<00:00, 119.99s/it]Evaluating: 100%|██████████| 28/28 [1:11:52<00:00, 154.03s/it]
Rank 0 :idx list:[array([ 0,  2,  4,  6,  8, 10, 12, 14]), array([16, 18, 20, 22, 24, 26, 28, 30]), array([32, 34, 36, 38, 40, 42, 44, 46]), array([48, 50, 52, 54, 56, 58, 60, 62]), array([64, 66, 68, 70, 72, 74, 76, 78]), array([80, 82, 84, 86, 88, 90, 92, 94]), array([ 96,  98, 100, 102, 104, 106, 108, 110]), array([112, 114, 116, 118, 120, 122, 124, 126]), array([128, 130, 132, 134, 136, 138, 140, 142]), array([144, 146, 148, 150, 152, 154, 156, 158]), array([160, 162, 164, 166, 168, 170, 172, 174]), array([176, 178, 180, 182, 184, 186, 188, 190]), array([192, 194, 196, 198, 200, 202, 204, 206]), array([208, 210, 212, 214, 216, 218, 220, 222]), array([224, 226, 228, 230, 232, 234, 236, 238]), array([240, 242, 244, 246, 248, 250, 252, 254]), array([256, 258, 260, 262, 264, 266, 268, 270]), array([272, 274, 276, 278, 280, 282, 284, 286]), array([288, 290, 292, 294, 296, 298, 300, 302]), array([304, 306, 308, 310, 312, 314, 316, 318]), array([320, 322, 324, 326, 328, 330, 332, 334]), array([336, 338, 340, 342, 344, 346, 348, 350]), array([352, 354, 356, 358, 360, 362, 364, 366]), array([368, 370, 372, 374, 376, 378, 380, 382]), array([384, 386, 388, 390, 392, 394, 396, 398]), array([400, 402, 404, 406, 408, 410, 412, 414]), array([416, 418, 420, 422, 424, 426, 428, 430]), array([432])]
gather index:[  0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34
  36  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70
  72  74  76  78  80  82  84  86  88  90  92  94  96  98 100 102 104 106
 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142
 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178
 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214
 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250
 252 254 256 258 260 262 264 266 268 270 272 274 276 278 280 282 284 286
 288 290 292 294 296 298 300 302 304 306 308 310 312 314 316 318 320 322
 324 326 328 330 332 334 336 338 340 342 344 346 348 350 352 354 356 358
 360 362 364 366 368 370 372 374 376 378 380 382 384 386 388 390 392 394
 396 398 400 402 404 406 408 410 412 414 416 418 420 422 424 426 428 430
 432   1   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33
  35  37  39  41  43  45  47  49  51  53  55  57  59  61  63  65  67  69
  71  73  75  77  79  81  83  85  87  89  91  93  95  97  99 101 103 105
 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141
 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177
 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213
 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249
 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285
 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321
 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357
 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393
 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429
 431   0]
order:[  0 433 217   1 218   2 219   3 220   4 221   5 222   6 223   7 224   8
 225   9 226  10 227  11 228  12 229  13 230  14 231  15 232  16 233  17
 234  18 235  19 236  20 237  21 238  22 239  23 240  24 241  25 242  26
 243  27 244  28 245  29 246  30 247  31 248  32 249  33 250  34 251  35
 252  36 253  37 254  38 255  39 256  40 257  41 258  42 259  43 260  44
 261  45 262  46 263  47 264  48 265  49 266  50 267  51 268  52 269  53
 270  54 271  55 272  56 273  57 274  58 275  59 276  60 277  61 278  62
 279  63 280  64 281  65 282  66 283  67 284  68 285  69 286  70 287  71
 288  72 289  73 290  74 291  75 292  76 293  77 294  78 295  79 296  80
 297  81 298  82 299  83 300  84 301  85 302  86 303  87 304  88 305  89
 306  90 307  91 308  92 309  93 310  94 311  95 312  96 313  97 314  98
 315  99 316 100 317 101 318 102 319 103 320 104 321 105 322 106 323 107
 324 108 325 109 326 110 327 111 328 112 329 113 330 114 331 115 332 116
 333 117 334 118 335 119 336 120 337 121 338 122 339 123 340 124 341 125
 342 126 343 127 344 128 345 129 346 130 347 131 348 132 349 133 350 134
 351 135 352 136 353 137 354 138 355 139 356 140 357 141 358 142 359 143
 360 144 361 145 362 146 363 147 364 148 365 149 366 150 367 151 368 152
 369 153 370 154 371 155 372 156 373 157 374 158 375 159 376 160 377 161
 378 162 379 163 380 164 381 165 382 166 383 167 384 168 385 169 386 170
 387 171 388 172 389 173 390 174 391 175 392 176 393 177 394 178 395 179
 396 180 397 181 398 182 399 183 400 184 401 185 402 186 403 187 404 188
 405 189 406 190 407 191 408 192 409 193 410 194 411 195 412 196 413 197
 414 198 415 199 416 200 417 201 418 202 419 203 420 204 421 205 422 206
 423 207 424 208 425 209 426 210 427 211 428 212 429 213 430 214 431 215
 432 216]

已加载基线模型 (TimeXer) 预测，用于 DM 显著性检验。
all_baseline_preds shape:(412, 12, 275)
y_true shape:(412, 12, 275)
y_pred shape:(412, 12, 275)
y_samp shape:(412, 20, 12, 275)
all_baseline_shape:(412, 12, 275)
--- 2nd Best Val Loss Model 结果 ---

--- Overall Metrics ---
MAE:  0.0075
RMSE: 0.0156
RAE:  0.0599
MAPE: 4.57%
CRPS: 0.0055
-----------------------

--- Horizon-wise Metrics ---
   Horizon       MAE      RMSE       RAE      MAPE      CRPS
0      t+1  0.007484  0.015165  0.060098  4.632342  0.005531
1      t+2  0.008305  0.017425  0.066679  4.983346  0.006151
2      t+3  0.008230  0.017328  0.066087  4.882832  0.006068
3      t+4  0.008224  0.017189  0.066051  4.897734  0.006053
4      t+5  0.008211  0.017390  0.065953  4.910539  0.006025
5      t+6  0.008078  0.017008  0.064883  4.912754  0.005940
6      t+7  0.007599  0.015941  0.061029  4.684859  0.005636
7      t+8  0.007319  0.015440  0.058760  4.472096  0.005421
8      t+9  0.006938  0.014520  0.055667  4.192870  0.005137
9     t+10  0.006853  0.014276  0.054967  4.225257  0.005083
10    t+11  0.006849  0.013842  0.054922  4.365487  0.005107
11    t+12  0.005398  0.009642  0.043262  3.620464  0.003996
----------------------------

--- Significance Test (Diebold-Mariano) ---
Comparing Your Model against Naive Baseline:
DM Statistic: 558.5438, P-value: 0.0000000
Conclusion: Your model is STATISTICALLY SIGNIFICANTLY BETTER than the Naive baseline (p < 0.05).
--------------------------------------------


>>> 正在评估: Best Val MAE Model
    路径: ./weights/st_diffusion_model_v2_20251228_231040_mae_best.pth
Model loaded successfully.
Model loaded successfully.
Test dataset size: 433 samples.
Test dataset size: 433 samples.
Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]Evaluating:   4%|▎         | 1/28 [02:39<1:11:36, 159.13s/it]Evaluating:   7%|▋         | 2/28 [05:17<1:08:46, 158.71s/it]Evaluating:  11%|█         | 3/28 [07:56<1:06:10, 158.81s/it]Evaluating:  14%|█▍        | 4/28 [10:35<1:03:32, 158.87s/it]Evaluating:  18%|█▊        | 5/28 [13:14<1:00:54, 158.90s/it]Evaluating:  21%|██▏       | 6/28 [15:53<58:16, 158.92s/it]  Evaluating:  25%|██▌       | 7/28 [18:32<55:37, 158.93s/it]Evaluating:  29%|██▊       | 8/28 [21:11<52:58, 158.94s/it]Evaluating:  32%|███▏      | 9/28 [23:50<50:20, 158.95s/it]Evaluating:  36%|███▌      | 10/28 [26:29<47:41, 158.95s/it]Evaluating:  39%|███▉      | 11/28 [29:08<45:02, 158.95s/it]Evaluating:  43%|████▎     | 12/28 [31:47<42:23, 158.95s/it]Evaluating:  46%|████▋     | 13/28 [34:26<39:44, 158.95s/it]Evaluating:  50%|█████     | 14/28 [37:04<37:05, 158.95s/it]Evaluating:  54%|█████▎    | 15/28 [39:43<34:26, 158.95s/it]Evaluating:  57%|█████▋    | 16/28 [42:22<31:47, 158.95s/it]Evaluating:  61%|██████    | 17/28 [45:01<29:08, 158.95s/it]Evaluating:  64%|██████▍   | 18/28 [47:40<26:29, 158.95s/it]Evaluating:  68%|██████▊   | 19/28 [50:19<23:50, 158.95s/it]Evaluating:  71%|███████▏  | 20/28 [52:58<21:11, 158.96s/it]Evaluating:  75%|███████▌  | 21/28 [55:37<18:32, 158.96s/it]Evaluating:  79%|███████▊  | 22/28 [58:16<15:53, 158.97s/it]Evaluating:  82%|████████▏ | 23/28 [1:00:55<13:14, 158.97s/it]Evaluating:  86%|████████▌ | 24/28 [1:03:34<10:35, 158.96s/it]Evaluating:  89%|████████▉ | 25/28 [1:06:13<07:56, 158.96s/it]Evaluating:  93%|█████████▎| 26/28 [1:08:52<05:17, 158.96s/it]Evaluating:  96%|█████████▋| 27/28 [1:11:31<02:38, 158.96s/it]Rank 1 :idx list:[array([ 1,  3,  5,  7,  9, 11, 13, 15]), array([17, 19, 21, 23, 25, 27, 29, 31]), array([33, 35, 37, 39, 41, 43, 45, 47]), array([49, 51, 53, 55, 57, 59, 61, 63]), array([65, 67, 69, 71, 73, 75, 77, 79]), array([81, 83, 85, 87, 89, 91, 93, 95]), array([ 97,  99, 101, 103, 105, 107, 109, 111]), array([113, 115, 117, 119, 121, 123, 125, 127]), array([129, 131, 133, 135, 137, 139, 141, 143]), array([145, 147, 149, 151, 153, 155, 157, 159]), array([161, 163, 165, 167, 169, 171, 173, 175]), array([177, 179, 181, 183, 185, 187, 189, 191]), array([193, 195, 197, 199, 201, 203, 205, 207]), array([209, 211, 213, 215, 217, 219, 221, 223]), array([225, 227, 229, 231, 233, 235, 237, 239]), array([241, 243, 245, 247, 249, 251, 253, 255]), array([257, 259, 261, 263, 265, 267, 269, 271]), array([273, 275, 277, 279, 281, 283, 285, 287]), array([289, 291, 293, 295, 297, 299, 301, 303]), array([305, 307, 309, 311, 313, 315, 317, 319]), array([321, 323, 325, 327, 329, 331, 333, 335]), array([337, 339, 341, 343, 345, 347, 349, 351]), array([353, 355, 357, 359, 361, 363, 365, 367]), array([369, 371, 373, 375, 377, 379, 381, 383]), array([385, 387, 389, 391, 393, 395, 397, 399]), array([401, 403, 405, 407, 409, 411, 413, 415]), array([417, 419, 421, 423, 425, 427, 429, 431]), array([0])]
Evaluating: 100%|██████████| 28/28 [1:12:01<00:00, 120.35s/it]Evaluating: 100%|██████████| 28/28 [1:12:01<00:00, 154.35s/it]
Rank 0 :idx list:[array([ 0,  2,  4,  6,  8, 10, 12, 14]), array([16, 18, 20, 22, 24, 26, 28, 30]), array([32, 34, 36, 38, 40, 42, 44, 46]), array([48, 50, 52, 54, 56, 58, 60, 62]), array([64, 66, 68, 70, 72, 74, 76, 78]), array([80, 82, 84, 86, 88, 90, 92, 94]), array([ 96,  98, 100, 102, 104, 106, 108, 110]), array([112, 114, 116, 118, 120, 122, 124, 126]), array([128, 130, 132, 134, 136, 138, 140, 142]), array([144, 146, 148, 150, 152, 154, 156, 158]), array([160, 162, 164, 166, 168, 170, 172, 174]), array([176, 178, 180, 182, 184, 186, 188, 190]), array([192, 194, 196, 198, 200, 202, 204, 206]), array([208, 210, 212, 214, 216, 218, 220, 222]), array([224, 226, 228, 230, 232, 234, 236, 238]), array([240, 242, 244, 246, 248, 250, 252, 254]), array([256, 258, 260, 262, 264, 266, 268, 270]), array([272, 274, 276, 278, 280, 282, 284, 286]), array([288, 290, 292, 294, 296, 298, 300, 302]), array([304, 306, 308, 310, 312, 314, 316, 318]), array([320, 322, 324, 326, 328, 330, 332, 334]), array([336, 338, 340, 342, 344, 346, 348, 350]), array([352, 354, 356, 358, 360, 362, 364, 366]), array([368, 370, 372, 374, 376, 378, 380, 382]), array([384, 386, 388, 390, 392, 394, 396, 398]), array([400, 402, 404, 406, 408, 410, 412, 414]), array([416, 418, 420, 422, 424, 426, 428, 430]), array([432])]
gather index:[  0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34
  36  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70
  72  74  76  78  80  82  84  86  88  90  92  94  96  98 100 102 104 106
 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142
 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178
 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214
 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250
 252 254 256 258 260 262 264 266 268 270 272 274 276 278 280 282 284 286
 288 290 292 294 296 298 300 302 304 306 308 310 312 314 316 318 320 322
 324 326 328 330 332 334 336 338 340 342 344 346 348 350 352 354 356 358
 360 362 364 366 368 370 372 374 376 378 380 382 384 386 388 390 392 394
 396 398 400 402 404 406 408 410 412 414 416 418 420 422 424 426 428 430
 432   1   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33
  35  37  39  41  43  45  47  49  51  53  55  57  59  61  63  65  67  69
  71  73  75  77  79  81  83  85  87  89  91  93  95  97  99 101 103 105
 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141
 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177
 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213
 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249
 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285
 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321
 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357
 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393
 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429
 431   0]
order:[  0 433 217   1 218   2 219   3 220   4 221   5 222   6 223   7 224   8
 225   9 226  10 227  11 228  12 229  13 230  14 231  15 232  16 233  17
 234  18 235  19 236  20 237  21 238  22 239  23 240  24 241  25 242  26
 243  27 244  28 245  29 246  30 247  31 248  32 249  33 250  34 251  35
 252  36 253  37 254  38 255  39 256  40 257  41 258  42 259  43 260  44
 261  45 262  46 263  47 264  48 265  49 266  50 267  51 268  52 269  53
 270  54 271  55 272  56 273  57 274  58 275  59 276  60 277  61 278  62
 279  63 280  64 281  65 282  66 283  67 284  68 285  69 286  70 287  71
 288  72 289  73 290  74 291  75 292  76 293  77 294  78 295  79 296  80
 297  81 298  82 299  83 300  84 301  85 302  86 303  87 304  88 305  89
 306  90 307  91 308  92 309  93 310  94 311  95 312  96 313  97 314  98
 315  99 316 100 317 101 318 102 319 103 320 104 321 105 322 106 323 107
 324 108 325 109 326 110 327 111 328 112 329 113 330 114 331 115 332 116
 333 117 334 118 335 119 336 120 337 121 338 122 339 123 340 124 341 125
 342 126 343 127 344 128 345 129 346 130 347 131 348 132 349 133 350 134
 351 135 352 136 353 137 354 138 355 139 356 140 357 141 358 142 359 143
 360 144 361 145 362 146 363 147 364 148 365 149 366 150 367 151 368 152
 369 153 370 154 371 155 372 156 373 157 374 158 375 159 376 160 377 161
 378 162 379 163 380 164 381 165 382 166 383 167 384 168 385 169 386 170
 387 171 388 172 389 173 390 174 391 175 392 176 393 177 394 178 395 179
 396 180 397 181 398 182 399 183 400 184 401 185 402 186 403 187 404 188
 405 189 406 190 407 191 408 192 409 193 410 194 411 195 412 196 413 197
 414 198 415 199 416 200 417 201 418 202 419 203 420 204 421 205 422 206
 423 207 424 208 425 209 426 210 427 211 428 212 429 213 430 214 431 215
 432 216]

已加载基线模型 (TimeXer) 预测，用于 DM 显著性检验。
all_baseline_preds shape:(412, 12, 275)
y_true shape:(412, 12, 275)
y_pred shape:(412, 12, 275)
y_samp shape:(412, 20, 12, 275)
all_baseline_shape:(412, 12, 275)
--- Best Val MAE Model 结果 ---

--- Overall Metrics ---
MAE:  0.0070
RMSE: 0.0152
RAE:  0.0559
MAPE: 4.08%
CRPS: 0.0052
-----------------------

--- Horizon-wise Metrics ---
   Horizon       MAE      RMSE       RAE      MAPE      CRPS
0      t+1  0.006809  0.014493  0.054672  4.064784  0.005098
1      t+2  0.007881  0.017295  0.063282  4.489976  0.005879
2      t+3  0.007669  0.016921  0.061579  4.387118  0.005734
3      t+4  0.007807  0.017050  0.062702  4.452223  0.005829
4      t+5  0.007655  0.016766  0.061483  4.360939  0.005665
5      t+6  0.007646  0.016741  0.061415  4.374779  0.005649
6      t+7  0.007148  0.015697  0.057407  4.122907  0.005301
7      t+8  0.006869  0.015186  0.055145  3.977797  0.005109
8      t+9  0.006539  0.014223  0.052466  3.817272  0.004882
9     t+10  0.006384  0.013860  0.051209  3.805139  0.004770
10    t+11  0.006334  0.013185  0.050788  3.885925  0.004751
11    t+12  0.004900  0.009014  0.039268  3.188458  0.003651
----------------------------

--- Significance Test (Diebold-Mariano) ---
Comparing Your Model against Naive Baseline:
DM Statistic: 566.6528, P-value: 0.0000000
Conclusion: Your model is STATISTICALLY SIGNIFICANTLY BETTER than the Naive baseline (p < 0.05).
--------------------------------------------


>>> 正在评估: 2nd Best Val MAE Model
    路径: ./weights/st_diffusion_model_v2_20251228_231040_mae_second_best.pth
Model loaded successfully.
Model loaded successfully.
Test dataset size: 433 samples.
Test dataset size: 433 samples.
Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]Evaluating:   4%|▎         | 1/28 [02:39<1:11:34, 159.04s/it]Evaluating:   7%|▋         | 2/28 [05:17<1:08:45, 158.67s/it]Evaluating:  11%|█         | 3/28 [07:55<1:06:05, 158.61s/it]Evaluating:  14%|█▍        | 4/28 [10:34<1:03:26, 158.59s/it]Evaluating:  18%|█▊        | 5/28 [13:13<1:00:47, 158.58s/it]Evaluating:  21%|██▏       | 6/28 [15:51<58:08, 158.57s/it]  Evaluating:  25%|██▌       | 7/28 [18:30<55:30, 158.57s/it]Evaluating:  29%|██▊       | 8/28 [21:08<52:51, 158.57s/it]Evaluating:  32%|███▏      | 9/28 [23:47<50:12, 158.57s/it]Evaluating:  36%|███▌      | 10/28 [26:25<47:34, 158.57s/it]Evaluating:  39%|███▉      | 11/28 [29:04<44:55, 158.57s/it]Evaluating:  43%|████▎     | 12/28 [31:43<42:16, 158.55s/it]Evaluating:  46%|████▋     | 13/28 [34:21<39:38, 158.59s/it]Evaluating:  50%|█████     | 14/28 [37:00<37:02, 158.73s/it]Evaluating:  54%|█████▎    | 15/28 [39:39<34:24, 158.82s/it]Evaluating:  57%|█████▋    | 16/28 [42:18<31:46, 158.89s/it]Evaluating:  61%|██████    | 17/28 [44:57<29:08, 158.94s/it]Evaluating:  64%|██████▍   | 18/28 [47:36<26:29, 158.97s/it]Evaluating:  68%|██████▊   | 19/28 [50:15<23:50, 158.98s/it]Evaluating:  71%|███████▏  | 20/28 [52:54<21:11, 158.99s/it]Evaluating:  75%|███████▌  | 21/28 [55:33<18:33, 159.01s/it]Evaluating:  79%|███████▊  | 22/28 [58:13<15:54, 159.02s/it]Evaluating:  82%|████████▏ | 23/28 [1:00:52<13:15, 159.03s/it]Evaluating:  86%|████████▌ | 24/28 [1:03:31<10:36, 159.04s/it]Evaluating:  89%|████████▉ | 25/28 [1:06:10<07:57, 159.04s/it]Evaluating:  93%|█████████▎| 26/28 [1:08:49<05:18, 159.05s/it]Rank 1 :idx list:[array([ 1,  3,  5,  7,  9, 11, 13, 15]), array([17, 19, 21, 23, 25, 27, 29, 31]), array([33, 35, 37, 39, 41, 43, 45, 47]), array([49, 51, 53, 55, 57, 59, 61, 63]), array([65, 67, 69, 71, 73, 75, 77, 79]), array([81, 83, 85, 87, 89, 91, 93, 95]), array([ 97,  99, 101, 103, 105, 107, 109, 111]), array([113, 115, 117, 119, 121, 123, 125, 127]), array([129, 131, 133, 135, 137, 139, 141, 143]), array([145, 147, 149, 151, 153, 155, 157, 159]), array([161, 163, 165, 167, 169, 171, 173, 175]), array([177, 179, 181, 183, 185, 187, 189, 191]), array([193, 195, 197, 199, 201, 203, 205, 207]), array([209, 211, 213, 215, 217, 219, 221, 223]), array([225, 227, 229, 231, 233, 235, 237, 239]), array([241, 243, 245, 247, 249, 251, 253, 255]), array([257, 259, 261, 263, 265, 267, 269, 271]), array([273, 275, 277, 279, 281, 283, 285, 287]), array([289, 291, 293, 295, 297, 299, 301, 303]), array([305, 307, 309, 311, 313, 315, 317, 319]), array([321, 323, 325, 327, 329, 331, 333, 335]), array([337, 339, 341, 343, 345, 347, 349, 351]), array([353, 355, 357, 359, 361, 363, 365, 367]), array([369, 371, 373, 375, 377, 379, 381, 383]), array([385, 387, 389, 391, 393, 395, 397, 399]), array([401, 403, 405, 407, 409, 411, 413, 415]), array([417, 419, 421, 423, 425, 427, 429, 431]), array([0])]
Evaluating:  96%|█████████▋| 27/28 [1:11:28<02:39, 159.02s/it]Evaluating: 100%|██████████| 28/28 [1:11:58<00:00, 120.40s/it]Evaluating: 100%|██████████| 28/28 [1:11:58<00:00, 154.23s/it]
Rank 0 :idx list:[array([ 0,  2,  4,  6,  8, 10, 12, 14]), array([16, 18, 20, 22, 24, 26, 28, 30]), array([32, 34, 36, 38, 40, 42, 44, 46]), array([48, 50, 52, 54, 56, 58, 60, 62]), array([64, 66, 68, 70, 72, 74, 76, 78]), array([80, 82, 84, 86, 88, 90, 92, 94]), array([ 96,  98, 100, 102, 104, 106, 108, 110]), array([112, 114, 116, 118, 120, 122, 124, 126]), array([128, 130, 132, 134, 136, 138, 140, 142]), array([144, 146, 148, 150, 152, 154, 156, 158]), array([160, 162, 164, 166, 168, 170, 172, 174]), array([176, 178, 180, 182, 184, 186, 188, 190]), array([192, 194, 196, 198, 200, 202, 204, 206]), array([208, 210, 212, 214, 216, 218, 220, 222]), array([224, 226, 228, 230, 232, 234, 236, 238]), array([240, 242, 244, 246, 248, 250, 252, 254]), array([256, 258, 260, 262, 264, 266, 268, 270]), array([272, 274, 276, 278, 280, 282, 284, 286]), array([288, 290, 292, 294, 296, 298, 300, 302]), array([304, 306, 308, 310, 312, 314, 316, 318]), array([320, 322, 324, 326, 328, 330, 332, 334]), array([336, 338, 340, 342, 344, 346, 348, 350]), array([352, 354, 356, 358, 360, 362, 364, 366]), array([368, 370, 372, 374, 376, 378, 380, 382]), array([384, 386, 388, 390, 392, 394, 396, 398]), array([400, 402, 404, 406, 408, 410, 412, 414]), array([416, 418, 420, 422, 424, 426, 428, 430]), array([432])]
gather index:[  0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34
  36  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70
  72  74  76  78  80  82  84  86  88  90  92  94  96  98 100 102 104 106
 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142
 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178
 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214
 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250
 252 254 256 258 260 262 264 266 268 270 272 274 276 278 280 282 284 286
 288 290 292 294 296 298 300 302 304 306 308 310 312 314 316 318 320 322
 324 326 328 330 332 334 336 338 340 342 344 346 348 350 352 354 356 358
 360 362 364 366 368 370 372 374 376 378 380 382 384 386 388 390 392 394
 396 398 400 402 404 406 408 410 412 414 416 418 420 422 424 426 428 430
 432   1   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33
  35  37  39  41  43  45  47  49  51  53  55  57  59  61  63  65  67  69
  71  73  75  77  79  81  83  85  87  89  91  93  95  97  99 101 103 105
 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141
 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177
 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213
 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249
 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285
 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321
 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357
 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393
 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429
 431   0]
order:[  0 433 217   1 218   2 219   3 220   4 221   5 222   6 223   7 224   8
 225   9 226  10 227  11 228  12 229  13 230  14 231  15 232  16 233  17
 234  18 235  19 236  20 237  21 238  22 239  23 240  24 241  25 242  26
 243  27 244  28 245  29 246  30 247  31 248  32 249  33 250  34 251  35
 252  36 253  37 254  38 255  39 256  40 257  41 258  42 259  43 260  44
 261  45 262  46 263  47 264  48 265  49 266  50 267  51 268  52 269  53
 270  54 271  55 272  56 273  57 274  58 275  59 276  60 277  61 278  62
 279  63 280  64 281  65 282  66 283  67 284  68 285  69 286  70 287  71
 288  72 289  73 290  74 291  75 292  76 293  77 294  78 295  79 296  80
 297  81 298  82 299  83 300  84 301  85 302  86 303  87 304  88 305  89
 306  90 307  91 308  92 309  93 310  94 311  95 312  96 313  97 314  98
 315  99 316 100 317 101 318 102 319 103 320 104 321 105 322 106 323 107
 324 108 325 109 326 110 327 111 328 112 329 113 330 114 331 115 332 116
 333 117 334 118 335 119 336 120 337 121 338 122 339 123 340 124 341 125
 342 126 343 127 344 128 345 129 346 130 347 131 348 132 349 133 350 134
 351 135 352 136 353 137 354 138 355 139 356 140 357 141 358 142 359 143
 360 144 361 145 362 146 363 147 364 148 365 149 366 150 367 151 368 152
 369 153 370 154 371 155 372 156 373 157 374 158 375 159 376 160 377 161
 378 162 379 163 380 164 381 165 382 166 383 167 384 168 385 169 386 170
 387 171 388 172 389 173 390 174 391 175 392 176 393 177 394 178 395 179
 396 180 397 181 398 182 399 183 400 184 401 185 402 186 403 187 404 188
 405 189 406 190 407 191 408 192 409 193 410 194 411 195 412 196 413 197
 414 198 415 199 416 200 417 201 418 202 419 203 420 204 421 205 422 206
 423 207 424 208 425 209 426 210 427 211 428 212 429 213 430 214 431 215
 432 216]

已加载基线模型 (TimeXer) 预测，用于 DM 显著性检验。
all_baseline_preds shape:(412, 12, 275)
y_true shape:(412, 12, 275)
y_pred shape:(412, 12, 275)
y_samp shape:(412, 20, 12, 275)
all_baseline_shape:(412, 12, 275)
--- 2nd Best Val MAE Model 结果 ---

--- Overall Metrics ---
MAE:  0.0070
RMSE: 0.0152
RAE:  0.0560
MAPE: 4.07%
CRPS: 0.0052
-----------------------

--- Horizon-wise Metrics ---
   Horizon       MAE      RMSE       RAE      MAPE      CRPS
0      t+1  0.006812  0.014511  0.054702  4.043270  0.005095
1      t+2  0.007862  0.017231  0.063129  4.456697  0.005876
2      t+3  0.007662  0.016921  0.061523  4.368502  0.005732
3      t+4  0.007804  0.017093  0.062672  4.441444  0.005811
4      t+5  0.007691  0.016867  0.061773  4.370820  0.005685
5      t+6  0.007654  0.016880  0.061473  4.358228  0.005653
6      t+7  0.007140  0.015686  0.057346  4.112887  0.005302
7      t+8  0.006888  0.015257  0.055299  3.969627  0.005123
8      t+9  0.006544  0.014220  0.052510  3.800730  0.004886
9     t+10  0.006424  0.013937  0.051526  3.826574  0.004793
10    t+11  0.006350  0.013293  0.050919  3.916650  0.004778
11    t+12  0.004898  0.009040  0.039258  3.183071  0.003655
----------------------------

--- Significance Test (Diebold-Mariano) ---
Comparing Your Model against Naive Baseline:
DM Statistic: 566.3840, P-value: 0.0000000
Conclusion: Your model is STATISTICALLY SIGNIFICANTLY BETTER than the Naive baseline (p < 0.05).
--------------------------------------------


>>> 正在评估: Best Val Loss Model
    路径: ./weights/st_diffusion_model_v2_20251228_231040_best.pth
Model loaded successfully.
Model loaded successfully.
Test dataset size: 433 samples.
Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]Test dataset size: 433 samples.
Evaluating:   4%|▎         | 1/28 [02:39<1:11:40, 159.29s/it]Evaluating:   7%|▋         | 2/28 [05:17<1:08:49, 158.83s/it]Evaluating:  11%|█         | 3/28 [07:56<1:06:06, 158.67s/it]Evaluating:  14%|█▍        | 4/28 [10:34<1:03:26, 158.60s/it]Evaluating:  18%|█▊        | 5/28 [13:13<1:00:47, 158.57s/it]Evaluating:  21%|██▏       | 6/28 [15:51<58:07, 158.54s/it]  Evaluating:  25%|██▌       | 7/28 [18:30<55:31, 158.65s/it]Evaluating:  29%|██▊       | 8/28 [21:09<52:55, 158.78s/it]Evaluating:  32%|███▏      | 9/28 [23:48<50:16, 158.79s/it]Evaluating:  36%|███▌      | 10/28 [26:27<47:36, 158.70s/it]Evaluating:  39%|███▉      | 11/28 [29:05<44:56, 158.64s/it]Evaluating:  43%|████▎     | 12/28 [31:44<42:17, 158.61s/it]Evaluating:  46%|████▋     | 13/28 [34:22<39:39, 158.62s/it]Evaluating:  50%|█████     | 14/28 [37:01<37:02, 158.76s/it]Evaluating:  54%|█████▎    | 15/28 [39:40<34:25, 158.87s/it]Evaluating:  57%|█████▋    | 16/28 [42:20<31:47, 158.95s/it]Evaluating:  61%|██████    | 17/28 [44:59<29:08, 159.00s/it]Evaluating:  64%|██████▍   | 18/28 [47:38<26:30, 159.03s/it]Evaluating:  68%|██████▊   | 19/28 [50:17<23:51, 159.05s/it]Evaluating:  71%|███████▏  | 20/28 [52:56<21:12, 159.07s/it]Evaluating:  75%|███████▌  | 21/28 [55:35<18:33, 159.08s/it]Evaluating:  79%|███████▊  | 22/28 [58:14<15:54, 159.08s/it]Evaluating:  82%|████████▏ | 23/28 [1:00:53<13:15, 159.09s/it]Evaluating:  86%|████████▌ | 24/28 [1:03:32<10:36, 159.09s/it]Evaluating:  89%|████████▉ | 25/28 [1:06:11<07:56, 158.97s/it]Evaluating:  93%|█████████▎| 26/28 [1:08:50<05:17, 158.85s/it]Evaluating:  96%|█████████▋| 27/28 [1:11:28<02:38, 158.75s/it]Rank 1 :idx list:[array([ 1,  3,  5,  7,  9, 11, 13, 15]), array([17, 19, 21, 23, 25, 27, 29, 31]), array([33, 35, 37, 39, 41, 43, 45, 47]), array([49, 51, 53, 55, 57, 59, 61, 63]), array([65, 67, 69, 71, 73, 75, 77, 79]), array([81, 83, 85, 87, 89, 91, 93, 95]), array([ 97,  99, 101, 103, 105, 107, 109, 111]), array([113, 115, 117, 119, 121, 123, 125, 127]), array([129, 131, 133, 135, 137, 139, 141, 143]), array([145, 147, 149, 151, 153, 155, 157, 159]), array([161, 163, 165, 167, 169, 171, 173, 175]), array([177, 179, 181, 183, 185, 187, 189, 191]), array([193, 195, 197, 199, 201, 203, 205, 207]), array([209, 211, 213, 215, 217, 219, 221, 223]), array([225, 227, 229, 231, 233, 235, 237, 239]), array([241, 243, 245, 247, 249, 251, 253, 255]), array([257, 259, 261, 263, 265, 267, 269, 271]), array([273, 275, 277, 279, 281, 283, 285, 287]), array([289, 291, 293, 295, 297, 299, 301, 303]), array([305, 307, 309, 311, 313, 315, 317, 319]), array([321, 323, 325, 327, 329, 331, 333, 335]), array([337, 339, 341, 343, 345, 347, 349, 351]), array([353, 355, 357, 359, 361, 363, 365, 367]), array([369, 371, 373, 375, 377, 379, 381, 383]), array([385, 387, 389, 391, 393, 395, 397, 399]), array([401, 403, 405, 407, 409, 411, 413, 415]), array([417, 419, 421, 423, 425, 427, 429, 431]), array([0])]
Evaluating: 100%|██████████| 28/28 [1:11:58<00:00, 120.23s/it]Evaluating: 100%|██████████| 28/28 [1:11:58<00:00, 154.25s/it]
Rank 0 :idx list:[array([ 0,  2,  4,  6,  8, 10, 12, 14]), array([16, 18, 20, 22, 24, 26, 28, 30]), array([32, 34, 36, 38, 40, 42, 44, 46]), array([48, 50, 52, 54, 56, 58, 60, 62]), array([64, 66, 68, 70, 72, 74, 76, 78]), array([80, 82, 84, 86, 88, 90, 92, 94]), array([ 96,  98, 100, 102, 104, 106, 108, 110]), array([112, 114, 116, 118, 120, 122, 124, 126]), array([128, 130, 132, 134, 136, 138, 140, 142]), array([144, 146, 148, 150, 152, 154, 156, 158]), array([160, 162, 164, 166, 168, 170, 172, 174]), array([176, 178, 180, 182, 184, 186, 188, 190]), array([192, 194, 196, 198, 200, 202, 204, 206]), array([208, 210, 212, 214, 216, 218, 220, 222]), array([224, 226, 228, 230, 232, 234, 236, 238]), array([240, 242, 244, 246, 248, 250, 252, 254]), array([256, 258, 260, 262, 264, 266, 268, 270]), array([272, 274, 276, 278, 280, 282, 284, 286]), array([288, 290, 292, 294, 296, 298, 300, 302]), array([304, 306, 308, 310, 312, 314, 316, 318]), array([320, 322, 324, 326, 328, 330, 332, 334]), array([336, 338, 340, 342, 344, 346, 348, 350]), array([352, 354, 356, 358, 360, 362, 364, 366]), array([368, 370, 372, 374, 376, 378, 380, 382]), array([384, 386, 388, 390, 392, 394, 396, 398]), array([400, 402, 404, 406, 408, 410, 412, 414]), array([416, 418, 420, 422, 424, 426, 428, 430]), array([432])]
gather index:[  0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34
  36  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70
  72  74  76  78  80  82  84  86  88  90  92  94  96  98 100 102 104 106
 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142
 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178
 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214
 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250
 252 254 256 258 260 262 264 266 268 270 272 274 276 278 280 282 284 286
 288 290 292 294 296 298 300 302 304 306 308 310 312 314 316 318 320 322
 324 326 328 330 332 334 336 338 340 342 344 346 348 350 352 354 356 358
 360 362 364 366 368 370 372 374 376 378 380 382 384 386 388 390 392 394
 396 398 400 402 404 406 408 410 412 414 416 418 420 422 424 426 428 430
 432   1   3   5   7   9  11  13  15  17  19  21  23  25  27  29  31  33
  35  37  39  41  43  45  47  49  51  53  55  57  59  61  63  65  67  69
  71  73  75  77  79  81  83  85  87  89  91  93  95  97  99 101 103 105
 107 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141
 143 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177
 179 181 183 185 187 189 191 193 195 197 199 201 203 205 207 209 211 213
 215 217 219 221 223 225 227 229 231 233 235 237 239 241 243 245 247 249
 251 253 255 257 259 261 263 265 267 269 271 273 275 277 279 281 283 285
 287 289 291 293 295 297 299 301 303 305 307 309 311 313 315 317 319 321
 323 325 327 329 331 333 335 337 339 341 343 345 347 349 351 353 355 357
 359 361 363 365 367 369 371 373 375 377 379 381 383 385 387 389 391 393
 395 397 399 401 403 405 407 409 411 413 415 417 419 421 423 425 427 429
 431   0]
order:[  0 433 217   1 218   2 219   3 220   4 221   5 222   6 223   7 224   8
 225   9 226  10 227  11 228  12 229  13 230  14 231  15 232  16 233  17
 234  18 235  19 236  20 237  21 238  22 239  23 240  24 241  25 242  26
 243  27 244  28 245  29 246  30 247  31 248  32 249  33 250  34 251  35
 252  36 253  37 254  38 255  39 256  40 257  41 258  42 259  43 260  44
 261  45 262  46 263  47 264  48 265  49 266  50 267  51 268  52 269  53
 270  54 271  55 272  56 273  57 274  58 275  59 276  60 277  61 278  62
 279  63 280  64 281  65 282  66 283  67 284  68 285  69 286  70 287  71
 288  72 289  73 290  74 291  75 292  76 293  77 294  78 295  79 296  80
 297  81 298  82 299  83 300  84 301  85 302  86 303  87 304  88 305  89
 306  90 307  91 308  92 309  93 310  94 311  95 312  96 313  97 314  98
 315  99 316 100 317 101 318 102 319 103 320 104 321 105 322 106 323 107
 324 108 325 109 326 110 327 111 328 112 329 113 330 114 331 115 332 116
 333 117 334 118 335 119 336 120 337 121 338 122 339 123 340 124 341 125
 342 126 343 127 344 128 345 129 346 130 347 131 348 132 349 133 350 134
 351 135 352 136 353 137 354 138 355 139 356 140 357 141 358 142 359 143
 360 144 361 145 362 146 363 147 364 148 365 149 366 150 367 151 368 152
 369 153 370 154 371 155 372 156 373 157 374 158 375 159 376 160 377 161
 378 162 379 163 380 164 381 165 382 166 383 167 384 168 385 169 386 170
 387 171 388 172 389 173 390 174 391 175 392 176 393 177 394 178 395 179
 396 180 397 181 398 182 399 183 400 184 401 185 402 186 403 187 404 188
 405 189 406 190 407 191 408 192 409 193 410 194 411 195 412 196 413 197
 414 198 415 199 416 200 417 201 418 202 419 203 420 204 421 205 422 206
 423 207 424 208 425 209 426 210 427 211 428 212 429 213 430 214 431 215
 432 216]

已加载基线模型 (TimeXer) 预测，用于 DM 显著性检验。
all_baseline_preds shape:(412, 12, 275)
y_true shape:(412, 12, 275)
y_pred shape:(412, 12, 275)
y_samp shape:(412, 20, 12, 275)
all_baseline_shape:(412, 12, 275)
--- Best Val Loss Model 结果 ---

--- Overall Metrics ---
MAE:  0.0070
RMSE: 0.0152
RAE:  0.0566
MAPE: 4.16%
CRPS: 0.0052
-----------------------

--- Horizon-wise Metrics ---
   Horizon       MAE      RMSE       RAE      MAPE      CRPS
0      t+1  0.006904  0.014524  0.055441  4.165261  0.005153
1      t+2  0.007924  0.017316  0.063621  4.541794  0.005902
2      t+3  0.007739  0.016842  0.062142  4.456464  0.005765
3      t+4  0.007892  0.017154  0.063377  4.515458  0.005848
4      t+5  0.007731  0.016747  0.062093  4.435516  0.005712
5      t+6  0.007714  0.016829  0.061960  4.454236  0.005694
6      t+7  0.007212  0.015654  0.057925  4.212588  0.005344
7      t+8  0.006960  0.015200  0.055878  4.063333  0.005162
8      t+9  0.006626  0.014310  0.053170  3.902542  0.004938
9     t+10  0.006476  0.013909  0.051945  3.879749  0.004824
10    t+11  0.006426  0.013234  0.051527  3.978175  0.004812
11    t+12  0.004989  0.009049  0.039982  3.292380  0.003706
----------------------------

--- Significance Test (Diebold-Mariano) ---
Comparing Your Model against Naive Baseline:
DM Statistic: 565.4922, P-value: 0.0000000
Conclusion: Your model is STATISTICALLY SIGNIFICANTLY BETTER than the Naive baseline (p < 0.05).
--------------------------------------------


============================================================
所有指定模型的测试已完成。
============================================================
