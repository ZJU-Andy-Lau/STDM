{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "from scipy.stats import t\n",
    "import gc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_predictions = np.load(\"results/pred_20251125_160242_second_best.npy\")\n",
    "ground_truths = np.load(\"results/ground_truths.npy\")\n",
    "model_samples = np.load(\"results/samples_20251125_160242_second_best.npy\")\n",
    "baseline_predictions = np.load(\"urbanev/TimeXer_predictions.npy\")\n",
    "baseline_predictions = np.concatenate([baseline_predictions[:, :, -1:], baseline_predictions], axis=-1)[:, :, :-1]\n",
    "baseline_predictions = baseline_predictions[:model_predictions.shape[0]]\n",
    "model_predictions = model_predictions[:baseline_predictions.shape[0]]\n",
    "model_samples = model_samples[:baseline_predictions.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred_median, y_pred_samples):\n",
    "    \n",
    "    y_pred_samples= y_pred_samples.astype(np.float16)\n",
    "    metrics = {}\n",
    "    metrics['mae'] = float(np.mean(np.abs(y_pred_median - y_true)))\n",
    "    metrics['rmse'] = float(np.sqrt(np.mean((y_pred_median - y_true)**2)))\n",
    "    rae_numerator = np.sum(np.abs(y_pred_median - y_true))\n",
    "    rae_denominator = np.sum(np.abs(np.mean(y_true) - y_true))\n",
    "    metrics['rae'] = float(rae_numerator / (rae_denominator + 1e-9))\n",
    "    non_zero_mask = y_true != 0\n",
    "    metrics['mape'] = float(np.mean(np.abs((y_pred_median[non_zero_mask] - y_true[non_zero_mask]) / y_true[non_zero_mask])) * 100) if np.any(non_zero_mask) else 0.0\n",
    "    gc.collect()\n",
    "    # metrics['crps'] = float(ps.crps_ensemble(y_true, y_pred_samples.transpose(1, 0, 2, 3), axis=0).mean())\n",
    "\n",
    "    horizon_metrics = []\n",
    "    for i in range(y_true.shape[1]): # 遍历预测长度\n",
    "        y_true_h, y_pred_median_h, y_pred_samples_h = y_true[:, i, :], y_pred_median[:, i, :], y_pred_samples[:, :, i, :]\n",
    "        mae_h = float(np.mean(np.abs(y_pred_median_h - y_true_h)))\n",
    "        rmse_h = float(np.sqrt(np.mean((y_pred_median_h - y_true_h)**2)))\n",
    "        rae_num_h = np.sum(np.abs(y_pred_median_h - y_true_h))\n",
    "        rae_den_h = np.sum(np.abs(np.mean(y_true_h) - y_true_h))\n",
    "        rae_h = float(rae_num_h / (rae_den_h + 1e-9))\n",
    "        non_zero_mask_h = y_true_h != 0\n",
    "        if np.any(non_zero_mask_h):\n",
    "            mape_h = float(np.mean(np.abs((y_pred_median_h[non_zero_mask_h] - y_true_h[non_zero_mask_h]) / y_true_h[non_zero_mask_h])) * 100)\n",
    "        else:\n",
    "            mape_h = 0.0\n",
    "        # crps_h = float(ps.crps_ensemble(y_true_h, y_pred_samples_h.transpose(1, 0, 2), axis=0).mean())\n",
    "        # horizon_metrics.append([f't+{i+1}', mae_h, rmse_h, rae_h, mape_h, crps_h])\n",
    "        horizon_metrics.append([f't+{i+1}', mae_h, rmse_h, rae_h, mape_h])\n",
    "\n",
    "    # df = pd.DataFrame(horizon_metrics, columns=['Horizon', 'MAE', 'RMSE', 'RAE', 'MAPE', 'CRPS'])\n",
    "    df = pd.DataFrame(horizon_metrics, columns=['Horizon', 'MAE', 'RMSE', 'RAE', 'MAPE'])\n",
    "    metrics['horizon_metrics'] = df.to_dict('records')\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(\"\\n--- Overall Metrics ---\")\n",
    "    print(f\"MAE:  {metrics['mae']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"RAE:  {metrics['rae']:.4f}\")\n",
    "    print(f\"MAPE: {metrics['mape']:.2f}%\")\n",
    "    # print(f\"CRPS: {metrics['crps']:.4f}\")\n",
    "    print(\"-----------------------\\n\")\n",
    "    \n",
    "    if 'horizon_metrics' in metrics:\n",
    "        horizon_df = pd.DataFrame(metrics['horizon_metrics'])\n",
    "        print(\"--- Horizon-wise Metrics ---\")\n",
    "        print(horizon_df)\n",
    "        print(\"----------------------------\\n\")\n",
    "\n",
    "    if 'dm_stat' in metrics:\n",
    "        print(\"--- Significance Test (Diebold-Mariano) ---\")\n",
    "        print(f\"Comparing Your Model against Naive Baseline:\")\n",
    "        print(f\"DM Statistic: {metrics['dm_stat']:.4f}, P-value: {metrics['p_value']:.7f}\")\n",
    "        if metrics['p_value'] < 0.05:\n",
    "            print(\"Conclusion: Your model is STATISTICALLY SIGNIFICANTLY BETTER than the Naive baseline (p < 0.05).\")\n",
    "        else:\n",
    "            print(\"Conclusion: No statistical evidence that your model is better than the Naive baseline (p >= 0.05).\")\n",
    "        print(\"--------------------------------------------\\n\")\n",
    "\n",
    "def dm_test(e1, e2, h=12, crit=\"MAD\"):\n",
    "    e1, e2 = np.array(e1), np.array(e2)\n",
    "    # d = np.abs(e1) - np.abs(e2)\n",
    "    if crit == \"MAD\":\n",
    "        d = np.abs(e1) - np.abs(e2)\n",
    "    elif crit == \"MSE\":\n",
    "        d = np.square(e1) - np.square(e2)\n",
    "    else:\n",
    "        # 如果传入不支持的参数，则报错\n",
    "        raise ValueError(f\"Unknown criterion: {crit}. Supported values are 'MAD' or 'MSE'.\")\n",
    "    d_mean = np.mean(d)\n",
    "    n = len(d)\n",
    "    d_centered = d - d_mean\n",
    "    acov = np.correlate(d_centered, d_centered, mode=\"full\")[n-1:n+h] / n\n",
    "    var_d = acov[0] + 2 * np.sum([(1 - lag/(h+1)) * acov[lag] for lag in range(1, h)])\n",
    "    var_d = max(var_d, 1e-12)\n",
    "    dm_stat = d_mean / np.sqrt(var_d / n)\n",
    "    p_value = 1 - t.cdf(dm_stat, df=n-1)\n",
    "    return dm_stat, p_value\n",
    "\n",
    "def calculate_pit(y_true, y_pred_samples):\n",
    "    \"\"\"\n",
    "    计算概率积分变换 (PIT) 值。\n",
    "    \n",
    "    这是评估概率预测校准度的核心工具。\n",
    "    它计算的是真实值 (y_true) 相对于预测样本 (y_pred_samples) 的经验分位数。\n",
    "    \n",
    "    参数:\n",
    "    y_true (np.ndarray): 真实值数组。\n",
    "                          形状: (B, L, N) 或 (num_datapoints,)\n",
    "    y_pred_samples (np.ndarray): 预测样本数组。\n",
    "                                  形状: (B, S, L, N) 或 (num_datapoints, S)\n",
    "                                  其中 S 是样本数量。\n",
    "\n",
    "    返回:\n",
    "    np.ndarray: 一维数组，包含所有数据点的 PIT 值 (范围在 0 到 1 之间)。\n",
    "                如果预测完美校准，此数组应服从均匀分布。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 为了进行广播比较，我们需要将 y_true 的形状扩展为 (B, 1, L, N)\n",
    "    # 以便它能和 (B, S, L, N) 的 y_pred_samples 进行比较。\n",
    "    if y_true.ndim == 3 and y_pred_samples.ndim == 4:\n",
    "        # 假设 y_true: (B, L, N), y_pred_samples: (B, S, L, N)\n",
    "        # B = 批量大小, L = 预测长度, N = 节点数, S = 样本数\n",
    "        y_true_expanded = np.expand_dims(y_true, axis=1) # 形状变为 (B, 1, L, N)\n",
    "        sample_axis = 1 # 样本在 S 维度上\n",
    "    elif y_true.ndim == 1 and y_pred_samples.ndim == 2:\n",
    "        # 假设 y_true: (N,), y_pred_samples: (N, S)\n",
    "        y_true_expanded = np.expand_dims(y_true, axis=1) # 形状变为 (N, 1)\n",
    "        sample_axis = 1 # 样本在 S 维度上\n",
    "    else:\n",
    "        raise ValueError(f\"输入形状不匹配！y_true: {y_true.shape}, y_pred_samples: {y_pred_samples.shape}\")\n",
    "\n",
    "    # (y_pred_samples <= y_true_expanded) 会生成一个布尔数组\n",
    "    # np.mean(..., axis=sample_axis) 会计算布尔值为 True 的比例，\n",
    "    # 这等同于 (count(samples <= true_value) / num_samples)\n",
    "    pit_values = np.mean(y_pred_samples <= y_true_expanded, axis=sample_axis)\n",
    "    \n",
    "    # 返回所有 PIT 值的一维数组，以便绘制直方图\n",
    "    return pit_values.flatten()\n",
    "\n",
    "def calculate_reliability_diagram(y_true, y_pred_samples, num_bins=10):\n",
    "    \"\"\"\n",
    "    计算可靠性图 (Reliability Diagram) 的数据点。\n",
    "\n",
    "    参数:\n",
    "    y_true (np.ndarray): 真实值数组。形状 (B, L, N) 或 (num_datapoints,)\n",
    "    y_pred_samples (np.ndarray): 预测样本数组。形状 (B, S, L, N) 或 (num_datapoints, S)\n",
    "    num_bins (int): 要检查的分位数“箱子”的数量 (例如 10，将检查 0.1, 0.2, ..., 1.0)。\n",
    "\n",
    "    返回:\n",
    "    tuple: (expected_quantiles, observed_frequencies)\n",
    "           - expected_quantiles (np.ndarray): 目标分位数 (例如 [0.1, 0.2, ..., 1.0])\n",
    "           - observed_frequencies (np.ndarray): 真实值低于该预测分位数的实际频率\n",
    "    \"\"\"\n",
    "    \n",
    "    if y_true.ndim == 3 and y_pred_samples.ndim == 4:\n",
    "        sample_axis = 1\n",
    "    elif y_true.ndim == 1 and y_pred_samples.ndim == 2:\n",
    "        sample_axis = 1\n",
    "    else:\n",
    "        raise ValueError(f\"输入形状不匹配！y_true: {y_true.shape}, y_pred_samples: {y_pred_samples.shape}\")\n",
    "\n",
    "    # 1. 定义我们想要检查的目标分位数（x轴）\n",
    "    # 例如 num_bins=10, 结果为 [0.1, 0.2, ..., 1.0]\n",
    "    expected_quantiles = np.linspace(0, 1, num_bins + 1)[1:]\n",
    "\n",
    "    # 2. 从预测样本中计算这些分位数的“预测值”\n",
    "    # q=expected_quantiles, axis=sample_axis\n",
    "    # predicted_quantiles 的形状: (Q, B, L, N) 或 (Q, N)\n",
    "    predicted_quantiles = np.quantile(y_pred_samples, q=expected_quantiles, axis=sample_axis)\n",
    "\n",
    "    # 3. 检查真实值 (y_true) 在多大频率上低于这些“预测值”\n",
    "    \n",
    "    # (B, L, N) -> (1, B, L, N) 或 (N,) -> (1, N)\n",
    "    y_true_expanded = np.expand_dims(y_true, axis=0) \n",
    "    \n",
    "    # (y_true_expanded <= predicted_quantiles)\n",
    "    # 广播比较 (1, B, L, N) <= (Q, B, L, N)\n",
    "    # 结果 observed_booleans 形状为 (Q, B, L, N)\n",
    "    observed_booleans = (y_true_expanded <= predicted_quantiles)\n",
    "    \n",
    "    # 沿着所有非分位数轴 (B, L, N) 计算均值，得到每个分位数的实际频率\n",
    "    data_axes = tuple(range(1, y_true_expanded.ndim)) # (1, 2, 3) 或 (1,)\n",
    "    observed_frequencies = np.mean(observed_booleans, axis=data_axes)\n",
    "    \n",
    "    return expected_quantiles, observed_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"--- 概率预测评估脚本 ---\")\n",
    "\n",
    "\n",
    "\n",
    "# 定义数据形状\n",
    "B = 500  # 批量大小 (数据点数量)\n",
    "L = 12   # 预测长度\n",
    "N = 275  # 节点数\n",
    "S = 40   # 样本数 (NUM_SAMPLES)\n",
    "\n",
    "\n",
    "# --- 2. 计算并绘制 PIT 直方图 ---\n",
    "print(\"\\n--- 正在计算 PIT 直方图 ---\")\n",
    "pit_values_calibrated = calculate_pit(ground_truths, model_samples)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(pit_values_calibrated, bins=20, kde=False, stat=\"density\", label=\"STDM模型分布\")\n",
    "plt.axhline(1.0, color='red', linestyle='--', label='完美校准 (均匀分布)')\n",
    "plt.title('PIT 直方图')\n",
    "plt.xlabel('PIT 值 (经验分位数)')\n",
    "plt.ylabel('密度')\n",
    "plt.legend()\n",
    "plt.ylim(0, 2) # 设置Y轴范围以便比较\n",
    "\n",
    "\n",
    "pit_hist_path = 'pit_histogram.png'\n",
    "# plt.savefig(pit_hist_path)\n",
    "print(f\"PIT 直方图已保存到: {pit_hist_path}\")\n",
    "\n",
    "# --- 3. 计算并绘制可靠性图 ---\n",
    "print(\"\\n--- 正在计算可靠性图 ---\")\n",
    "expected_q, observed_freq_calibrated = calculate_reliability_diagram(ground_truths, model_samples)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='perfect reliability line')\n",
    "plt.plot(expected_q, observed_freq_calibrated, 'bo-', label='STDM')\n",
    "plt.title('reliability diagram ')\n",
    "plt.xlabel('predicted quantiles')\n",
    "plt.ylabel('observed frequencies')\n",
    "plt.axis('square') # 确保x和y轴等比例\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "reliability_diag_path = 'reliability_diagram.png'\n",
    "# plt.savefig(reliability_diag_path)\n",
    "print(f\"可靠性图已保存到: {reliability_diag_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = calculate_metrics(ground_truths, model_predictions, model_samples)\n",
    "print_metrics(final_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('d2s')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ef1928c01dc9e302eecf4e58c03b6288f7565248e4f1b20cace10f92920a132"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
